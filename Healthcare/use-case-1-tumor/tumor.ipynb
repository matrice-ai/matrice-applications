{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tumor Classification Model Training\n",
    "\n",
    "In this notebook, we'll set up and train a tumor classification model using the Matrice SDK. The notebook includes steps to create a project, dataset, add models for training, and deploy the model.\n",
    "\n",
    "Make sure to replace placeholder values with your actual credentials and necessary parameters.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://test.pypi.org/simple/\n",
      "Collecting matrice_sdk\n",
      "  Using cached https://test-files.pythonhosted.org/packages/42/40/937b0e1f3656e1224cdcb3118c96d3340e870191f513983d32a4b332c23f/matrice_sdk-0.0.43-py3-none-any.whl (39 kB)\n",
      "Installing collected packages: matrice-sdk\n",
      "Successfully installed matrice-sdk-0.0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\pathi\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#!python -m pip install --upgrade --index-url https://test.pypi.org/simple/ --no-deps matrice_sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\pathi\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pathi\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pathi\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pathi\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pathi\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<matrice_sdk.matrice.Session object at 0x000001C96B5F7410>\n",
      "<matrice_sdk.projects.Projects object at 0x000001C96B953850>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set up environment\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(\"..\")\n",
    "sys.path.append(parent_dir)\n",
    "os.environ['ENV'] = \"DEV\"\n",
    "os.environ['MATRICE_SECRET_ACCESS_KEY'] = "\""\n",
    "os.environ['MATRICE_ACCESS_KEY_ID'] = "\""\n",
    "\n",
    "from matrice_sdk.projects import Projects\n",
    "from matrice_sdk.matrice import Session\n",
    "from matrice_sdk.dataset import Dataset\n",
    "from matrice_sdk.models import Model\n",
    "from matrice_sdk.deployment import Deployment\n",
    "from matrice_sdk.inference_optim import InferenceOptimization\n",
    "from matrice_sdk.model_prediction import ModelPrediction\n",
    "from matrice_sdk.user import User\n",
    "import time\n",
    "import threading\n",
    "\n",
    "# Initialize session and projects\n",
    "S = Session()\n",
    "print(S)\n",
    "P = Projects(S)\n",
    "print(P)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check Account Credits\n",
    "\n",
    "Let's check the available credits in the account to ensure there are enough resources for the operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "Initial credits: 8964222\n"
     ]
    }
   ],
   "source": [
    "def check_credit_deduction(account_number):\n",
    "    U = User(S)\n",
    "    resp, err, message = U.get_account_subscription(account_number)\n",
    "    if err is not None:\n",
    "        print(message)\n",
    "        sys.exit(1)\n",
    "    return resp[\"credits\"] + resp[\"subCredits\"]\n",
    "\n",
    "# Replace with your account number\n",
    "account_number = "''\n",
    "initial_credits = check_credit_deduction(account_number)\n",
    "print(f\"Initial credits: {initial_credits}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Project\n",
    "\n",
    "Create a project for tumor classification. This step will set up the project environment for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "Project created with ID: 66b1c1b399fbad9c1422cb4e\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to create a project\n",
    "def create_project(account_number):\n",
    "    project_name = \"tumor_classification_project_1.1\"\n",
    "    input_type = \"image\"\n",
    "    output_type = \"classification\"\n",
    "    enabled_platforms = {\n",
    "        \"matrice\": True,\n",
    "        \"android\": False,\n",
    "        \"ios\": False,\n",
    "        \"tpu\": False,\n",
    "        \"intelCPU\": False,\n",
    "        \"gcloudGPU\": False\n",
    "    }\n",
    "\n",
    "    resp, err, message = P.create_project(project_name, input_type, output_type, enabled_platforms, account_number)\n",
    "    if err is not None:\n",
    "        print(message)\n",
    "        sys.exit(1)\n",
    "\n",
    "    project_id = P.project_id\n",
    "    S.project_id = project_id\n",
    "    return project_id\n",
    "\n",
    "# Create project\n",
    "account_number = '2276842692221978464767135'\n",
    "project_id = create_project(account_number)\n",
    "print(f\"Project created with ID: {project_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = '66b1c1b399fbad9c1422cb4e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\pathi\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\pathi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\pathi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\pathi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pathi\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed. The processed dataset is zipped and saved as 'processed_dataset.zip'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from shutil import copy2, rmtree\n",
    "\n",
    "# Define paths\n",
    "dataset_dir = 'C:/Users/pathi/OneDrive/Desktop/matriceai/python-sdk/src/use_cases/brain_tumor_dataset'  # Replace with your dataset path\n",
    "yes_dir = os.path.join(dataset_dir, 'yes')\n",
    "no_dir = os.path.join(dataset_dir, 'no')\n",
    "\n",
    "# Create directories for train, test, val splits\n",
    "base_dir = 'processed_dataset'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "val_dir = os.path.join(base_dir, 'val')\n",
    "\n",
    "for sub_dir in [train_dir, test_dir, val_dir]:\n",
    "    os.makedirs(os.path.join(sub_dir, 'tumour'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(sub_dir, 'no_tumour'), exist_ok=True)\n",
    "\n",
    "# Function to split and copy files\n",
    "def split_and_copy(src_dir, dst_dirs, split_ratio):\n",
    "    files = os.listdir(src_dir)\n",
    "    train_files, val_test_files = train_test_split(files, test_size=split_ratio[1] + split_ratio[2], random_state=42)\n",
    "    val_files, test_files = train_test_split(val_test_files, test_size=split_ratio[2] / (split_ratio[1] + split_ratio[2]), random_state=42)\n",
    "\n",
    "    for f in train_files:\n",
    "        copy2(os.path.join(src_dir, f), dst_dirs[0])\n",
    "    for f in val_files:\n",
    "        copy2(os.path.join(src_dir, f), dst_dirs[1])\n",
    "    for f in test_files:\n",
    "        copy2(os.path.join(src_dir, f), dst_dirs[2])\n",
    "\n",
    "# Split and copy the 'yes' class\n",
    "split_and_copy(yes_dir, [os.path.join(train_dir, 'tumour'), os.path.join(val_dir, 'tumour'), os.path.join(test_dir, 'tumour')], [0.7, 0.2, 0.1])\n",
    "\n",
    "# Split and copy the 'no' class\n",
    "split_and_copy(no_dir, [os.path.join(train_dir, 'no_tumour'), os.path.join(val_dir, 'no_tumour'), os.path.join(test_dir, 'no_tumour')], [0.7, 0.2, 0.1])\n",
    "\n",
    "# Zip the processed dataset\n",
    "zip_filename = 'processed_dataset.zip'\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            zipf.write(os.path.join(root, file))\n",
    "\n",
    "# Clean up the original dataset directory if necessary\n",
    "# Uncomment the next line if you want to remove the original dataset directory\n",
    "# rmtree(dataset_dir)\n",
    "\n",
    "print(\"Preprocessing completed. The processed dataset is zipped and saved as 'processed_dataset.zip'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset\n",
    "\n",
    "Upload a dataset for tumor classification. Here, we use a placeholder URL for demonstration purposes. Replace it with actual dataset URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "ACK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "OK\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "SUCCESS\n",
      "Dataset created successfully\n",
      "Dataset created with ID: 66b1c1dcae5a8a699aec6887\n"
     ]
    }
   ],
   "source": [
    "# Function to create a dataset\n",
    "def create_dataset(project_id):\n",
    "    D = Dataset(S)\n",
    "    dataset_name = \"tumor_classification_dataset_2\"\n",
    "    dataset_url = \"https://s3.us-west-2.amazonaws.com/testing.resources/datasets/tumour_dataset.zip\"\n",
    "    is_unlabeled = False\n",
    "    source = \"url\"\n",
    "    dataset_description = \"Tumor classification dataset\"\n",
    "    version_description = \"v1.0\"\n",
    "\n",
    "    resp, err, message = D.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        source=source,\n",
    "        source_url=dataset_url,\n",
    "        dataset_type=\"classification\",\n",
    "        cloud_provider = \"AWS\",\n",
    "        dataset_description=dataset_description,\n",
    "        version_description=version_description,\n",
    "        input_type=\"image\"\n",
    "    )\n",
    "    if err is not None:\n",
    "        print(message)\n",
    "        sys.exit(1)\n",
    "\n",
    "    dataset_id = resp['_id']\n",
    "    dataset_version = \"v1.0\"\n",
    "\n",
    "    # Wait until dataset creation is successful\n",
    "    status = None\n",
    "    wait_time = 3 * 60\n",
    "    time.sleep(1 * 20)\n",
    "    while status != 'SUCCESS' and status != 'ERROR':\n",
    "        resp, _, _ = P.get_latest_action_record(dataset_id)\n",
    "        status = resp[\"data\"][\"status\"]\n",
    "        print(status)\n",
    "        time.sleep(wait_time)\n",
    "        wait_time /= 2\n",
    "        wait_time = max(30, wait_time)\n",
    "    \n",
    "    if status == 'SUCCESS':\n",
    "        print(\"Dataset created successfully\")\n",
    "    else:\n",
    "        print(\"Error in creating dataset\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    return dataset_id, dataset_version, dataset_name\n",
    "\n",
    "# Create dataset\n",
    "dataset_id, dataset_version, dataset_name = create_dataset(project_id)\n",
    "print(f\"Dataset created with ID: {dataset_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset_id, dataset_version, dataset_name\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Create dataset\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m dataset_id, dataset_version, dataset_name \u001b[38;5;241m=\u001b[39m create_dataset(project_id)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset created with ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[1;34m(project_id)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message)\n\u001b[1;32m---> 23\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m dataset_id \u001b[38;5;241m=\u001b[39m resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     26\u001b[0m dataset_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add Models for Training\n",
    "\n",
    "Add a model to the project for training. For tumor classification, we'll use a ResNet model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n",
      "Getting Auth bearer token...\n",
      "Setting bearer token...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_id\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Add models for training\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m model_id \u001b[38;5;241m=\u001b[39m add_models_for_training(project_id, dataset_id, dataset_version, dataset_name)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel added with ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m, in \u001b[0;36madd_models_for_training\u001b[1;34m(project_id, dataset_id, dataset_version, dataset_name)\u001b[0m\n\u001b[0;32m      6\u001b[0m primary_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m matrice_compute \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m exp_resp, err, message \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39mcreate_experiment(\n\u001b[0;32m     10\u001b[0m     name,\n\u001b[0;32m     11\u001b[0m     dataset_id,\n\u001b[0;32m     12\u001b[0m     target_run_time[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     13\u001b[0m     dataset_version,\n\u001b[0;32m     14\u001b[0m     primary_metric,\n\u001b[0;32m     15\u001b[0m     matrice_compute\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\matriceai\\python-sdk\\src\\use_cases\\matrice_sdk\\models.py:175\u001b[0m, in \u001b[0;36mModel.create_experiment\u001b[1;34m(self, name, dataset_id, target_run_time, dataset_version, primary_metric, matrice_compute, models_trained, performance_trade_off, project_id)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_info \u001b[38;5;129;01min\u001b[39;00m dataset_info:\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dataset_id\u001b[38;5;241m==\u001b[39mdata_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 175\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dataset_version \u001b[38;5;129;01min\u001b[39;00m data_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessedVersions\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    176\u001b[0m             model_information\u001b[38;5;241m=\u001b[39mdata_info\n\u001b[0;32m    177\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "# Function to add models for training\n",
    "def add_models_for_training(project_id, dataset_id, dataset_version, dataset_name):\n",
    "    M = Model(S)\n",
    "    name = \"Tumor-Classification-Experiment\"\n",
    "    target_run_time = [\"ONNX\"]\n",
    "    primary_metric = \"precision\"\n",
    "    matrice_compute = True\n",
    "\n",
    "    exp_resp, err, message = M.create_experiment(\n",
    "        name,\n",
    "        dataset_id,\n",
    "        target_run_time[0],\n",
    "        dataset_version,\n",
    "        primary_metric,\n",
    "        matrice_compute\n",
    "    )\n",
    "    if err is not None:\n",
    "        print(message)\n",
    "        sys.exit(1)\n",
    "    print(exp_resp)\n",
    "    \n",
    "    global experiment_id\n",
    "    experiment_id = exp_resp[\"_id\"]\n",
    "    model_payload = [\n",
    "        {\n",
    "            \"model_key\": \"resnet152\",\n",
    "            \"is_autoML\": False,\n",
    "            \"tuning_type\": \"manual\",\n",
    "            \"primary_metric\": primary_metric,\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"params_millions\": 0,\n",
    "            \"experiment_name\": \"Tumor-Classification-Experiment\",\n",
    "            \"model_name\": \"ResNet-152\",\n",
    "            \"model_inputs\": [\"image\"],\n",
    "            \"model_outputs\": [\"classification\"],\n",
    "            \"target_runtime\": target_run_time,\n",
    "            \"id_dataset\": dataset_id,\n",
    "            \"dataset_version\": dataset_version,\n",
    "            \"id_model_info\": \"65b8d07e47ac273ab8fe515d\",\n",
    "            \"id_experiment\": experiment_id,\n",
    "            \"action_config\": {},\n",
    "            \"model_config\": {\n",
    "                \"min_delta\": [0.0001],\n",
    "                \"lr_gamma\": [0.1],\n",
    "                \"lr_min\": [0.00001],\n",
    "                \"learning_rate\": [0.001],\n",
    "                \"patience\": [5],\n",
    "                \"lr_step_size\": [10],\n",
    "                \"lr_scheduler\": [\"StepLR\"],\n",
    "                \"optimizer\": [\"AdamW\"],\n",
    "                \"weight_decay\": [0.0001],\n",
    "                \"momentum\": [0.95],\n",
    "                \"epochs\": [1],\n",
    "                \"batch_size\": [4]\n",
    "            },\n",
    "            \"compute_alias\": \"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    model_resp, err, message = M.add_model_train_list(model_payload)\n",
    "    print(model_resp)\n",
    "    if err is not None:\n",
    "        print(message)\n",
    "        sys.exit(1)\n",
    "\n",
    "    model_id = model_resp[0][\"_id\"]\n",
    "    print(model_id)\n",
    "\n",
    "    # Wait until model training is successful\n",
    "    status = None\n",
    "    wait_time = 5 * 60\n",
    "    time.sleep(1 * 20)\n",
    "    while status != 'SUCCESS' and status != 'ERROR':\n",
    "        resp, _, _ = P.get_latest_action_record(model_id)\n",
    "        status = resp[\"data\"][\"status\"]\n",
    "        print(status)\n",
    "        time.sleep(wait_time)\n",
    "        wait_time /= 2\n",
    "        wait_time = max(30, wait_time)\n",
    "    \n",
    "    return model_id\n",
    "\n",
    "# Add models for training\n",
    "model_id = add_models_for_training(project_id, dataset_id, dataset_version, dataset_name)\n",
    "print(f\"Model added with ID: {model_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Start model logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matrice_sdk.matrice import Session\n",
    "from matrice_sdk.model_logging import ModelLogging\n",
    "S = Session(project_id)\n",
    "ML = ModelLogging(S, model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Fetch and plot model log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch model training logs\n",
    "resp, error, message = ML.get_model_training_logs()\n",
    "print(\"Message:\", message)\n",
    "print(\"Error:\", error)\n",
    "print(\"Response:\")\n",
    "pprint.pprint(resp)\n",
    "\n",
    "# Check if the response is successful\n",
    "if resp['success']:\n",
    "    # Extract training and validation metrics\n",
    "    metrics = {'epoch': [], 'acc@1': [], 'acc@5': [], 'precision': []}\n",
    "    \n",
    "    for log in resp['data']:\n",
    "        epoch = log['epoch']\n",
    "        epoch_metrics = {metric['metricName']: metric['metricValue'] for metric in log['epochDetails']}\n",
    "        \n",
    "        metrics['epoch'].append(epoch)\n",
    "        metrics['acc@1'].append(epoch_metrics.get('acc@1', None))\n",
    "        metrics['acc@5'].append(epoch_metrics.get('acc@5', None))\n",
    "        metrics['precision'].append(epoch_metrics.get('precision', None))\n",
    "\n",
    "    # Convert to DataFrame for better handling\n",
    "    df = pd.DataFrame(metrics)\n",
    "\n",
    "    # Plotting the metrics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(df['epoch'], df['acc@1'], marker='o', label='acc@1')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('acc@1')\n",
    "    plt.title('Model Accuracy@1 Over Epochs')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(df['epoch'], df['acc@5'], marker='o', label='acc@5')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('acc@5')\n",
    "    plt.title('Model Accuracy@5 Over Epochs')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(df['epoch'], df['precision'], marker='o', label='Precision')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Model Precision Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Failed to fetch logs or no logs available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Export the Trained Model\n",
    "\n",
    "Export the trained model to a format suitable for deployment or further evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(model_id):\n",
    "    I = InferenceOptimization(S)\n",
    "    export_formats = [\"ONNX\"]\n",
    "    model_config = {\"simplify\": \"False\", \"dynamic\": \"False\"}\n",
    "    \n",
    "    # Add model export\n",
    "    resp, err, message = I.add_model_export(model_id, export_formats, model_config)\n",
    "    if err is not None:\n",
    "        print(message)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    model_export_id = resp.get(\"data\")\n",
    "    \n",
    "    # Check status of the export\n",
    "    status = None\n",
    "    wait_time = 5 * 60\n",
    "    time.sleep(1 * 20)\n",
    "    while status != 'SUCCESS' and status != 'ERROR':\n",
    "        resp, _, _ = P.get_latest_action_record(model_export_id)\n",
    "        status = resp[\"data\"][\"status\"]\n",
    "        print(status)\n",
    "        time.sleep(wait_time)\n",
    "        wait_time /= 2\n",
    "        wait_time = max(30, wait_time)\n",
    "    \n",
    "    if status == 'SUCCESS':\n",
    "        print(\"Model exported successfully.\")\n",
    "    else:\n",
    "        print(\"Error in exporting model.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Get model exports\n",
    "    resp, error, message = I.get_model_exports()\n",
    "    if error is not None:\n",
    "        print(message)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    export_id = resp[\"data\"][0][\"_id\"]\n",
    "    return export_id\n",
    "\n",
    "# Export the trained model\n",
    "model_export_id = export_model(model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Inference Optimization\n",
    "\n",
    "Optimize the exported model for inference, including setting up configurations and export formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_inference(export_id):\n",
    "    I = InferenceOptimization(S)\n",
    "    \n",
    "    # Check the status of the inference optimization\n",
    "    status = None\n",
    "    wait_time = 5 * 60\n",
    "    time.sleep(1 * 20)\n",
    "    while status != 'SUCCESS' and status != 'ERROR':\n",
    "        resp, _, _ = P.get_latest_action_record(export_id)\n",
    "        status = resp[\"data\"][\"status\"]\n",
    "        print(status)\n",
    "        time.sleep(wait_time)\n",
    "        wait_time /= 2\n",
    "        wait_time = max(30, wait_time)\n",
    "    \n",
    "    if status == 'SUCCESS':\n",
    "        print(\"Inference optimization completed successfully.\")\n",
    "    else:\n",
    "        print(\"Error in inference optimization.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Get model export details\n",
    "    resp, error, message = I.get_model_exports()\n",
    "    if error is not None:\n",
    "        print(message)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    optimized_export_id = resp[\"data\"][0][\"_id\"]\n",
    "    return optimized_export_id\n",
    "\n",
    "# Optimize the exported model\n",
    "optimized_export_id = optimize_inference(model_export_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Evaluate the Optimized Model\n",
    "\n",
    "Evaluate the optimized model to ensure it meets performance metrics using the specified evaluation settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(optimized_export_id, dataset_id, experiment_id):\n",
    "    I = InferenceOptimization(S)\n",
    "    \n",
    "    # Payload for model evaluation\n",
    "    is_optimized = False\n",
    "    is_pruned = False\n",
    "    runtime_framework = \"OpenVINO\"\n",
    "    dataset_version = \"v1.0\"\n",
    "    is_gpu_required = False\n",
    "    split_types = [\"val\", \"train\", \"test\"]\n",
    "    model_type = \"exported\"\n",
    "    \n",
    "    # Add model evaluation\n",
    "    resp, error, message = I.add_model_eval(\n",
    "        is_optimized, \n",
    "        is_pruned, \n",
    "        runtime_framework, \n",
    "        dataset_id, \n",
    "        experiment_id, \n",
    "        dataset_version, \n",
    "        is_gpu_required, \n",
    "        split_types, \n",
    "        model_type\n",
    "    )\n",
    "    if error is not None:\n",
    "        print(message)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Get evaluation results\n",
    "    M = Model(S)\n",
    "    resp, error, message = M.get_eval_result(dataset_id, dataset_version, split_types)\n",
    "    if error is not None:\n",
    "        print(message)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(\"Model evaluation results:\", resp)\n",
    "\n",
    "# Evaluate the optimized model\n",
    "evaluate_model(optimized_export_id, dataset_id, experiment_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deploy the Model\n",
    "\n",
    "Deploy the trained model to make predictions. This step sets up the model deployment and retrieves the authentication key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deployment(model_id):\n",
    "    Dep = Deployment(S)\n",
    "    deployment_name = \"Tumor Classification Deployment\"\n",
    "    resp, err, message = Dep.create_deployment(deployment_name, model_id)\n",
    "    if err is not None:\n",
    "        print(message)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    deployment_id = resp.get(\"data\")\n",
    "    auth_key = None\n",
    "\n",
    "    while True:\n",
    "        resp, err, message = Dep.get_deployment()\n",
    "        if err is not None:\n",
    "            print(message)\n",
    "            sys.exit(1)\n",
    "        if resp[\"data\"][\"status\"] == 'deployed':\n",
    "            auth_key = resp[\"data\"][\"authKeys\"][0][\"key\"]\n",
    "            break\n",
    "        time.sleep(1 * 60)\n",
    "\n",
    "    return deployment_id, auth_key\n",
    "\n",
    "# Create deployment and get authentication key\n",
    "deployment_id, auth_key = create_deployment(model_id)\n",
    "print(f\"Model deployed with ID: {deployment_id}\")\n",
    "print(f\"Authentication Key: {auth_key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make Predictions\n",
    "\n",
    "Use the deployed model to make predictions on sample images. Adjust the folder path as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(deployment_id, auth_key):\n",
    "    MP = ModelPrediction(S, deployment_id)\n",
    "    folder_abs_path = \"../images\"  # Adjust path to your image folder\n",
    "    image_files = os.listdir(folder_abs_path)\n",
    "    for file_name in image_files:\n",
    "        image_path = os.path.join(folder_abs_path, file_name)\n",
    "        _, err, message = MP.get_model_prediction(image_path, auth_key)\n",
    "        if err is not None:\n",
    "            print(message)\n",
    "            sys.exit(1)\n",
    "        time.sleep(2)\n",
    "\n",
    "# Make predictions using the deployed model\n",
    "make_predictions(deployment_id, auth_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clean Up\n",
    "\n",
    "Optionally, delete the project after completion to free up resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_project(project_id):\n",
    "    resp, err, message = P.delete_project(project_id)\n",
    "    if err is not None:\n",
    "        print(message)\n",
    "        sys.exit(1)\n",
    "    print(\"Project deleted successfully\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
