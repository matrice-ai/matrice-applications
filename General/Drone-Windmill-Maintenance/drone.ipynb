{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "class Vars:\n",
    "    _data = {}\n",
    "    _path = None\n",
    "\n",
    "    @classmethod\n",
    "    def initialize(cls, path):\n",
    "        cls._path = path\n",
    "        cls._data = cls.load()\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls):\n",
    "        if cls._path and os.path.exists(cls._path):\n",
    "            with open(cls._path, 'r') as f:\n",
    "                return yaml.safe_load(f)\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, key):\n",
    "        if key not in cls._data:\n",
    "            raise KeyError(f\"Variable {key} not found\")\n",
    "        return cls._data[key]\n",
    "\n",
    "    @classmethod\n",
    "    def set(cls, key, value):\n",
    "        cls._data[key] = value\n",
    "        with open(cls._path, 'w') as f:\n",
    "            yaml.safe_dump(cls._data, f, default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vars.initialize('vars.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vars.set(\"ENV\", \"dev\")\n",
    "Vars.set(\"MATRICE_ACCESS_KEY_ID\", \"IDAZ************MQO6\")\n",
    "Vars.set(\"MATRICE_SECRET_ACCESS_KEY\", \"52VR************ZY5N8\")\n",
    "Vars.set(\"MATRICE_ACCOUNT_NUMBER\", \"227684************7135\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['ENV'] = Vars.get('ENV')\n",
    "access_key = Vars.get('MATRICE_ACCESS_KEY_ID')\n",
    "secret_key = Vars.get('MATRICE_SECRET_ACCESS_KEY')\n",
    "account_number = Vars.get('MATRICE_ACCOUNT_NUMBER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matrice.session import Session\n",
    "from matrice.projects import Projects\n",
    "from matrice.dataset import Dataset\n",
    "from matrice.models import Model\n",
    "from matrice.deployment import Deployment\n",
    "from matrice.exported_model import ExportedModel\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_instance = Session.create_session(account_number=account_number, access_key=access_key, secret_key=secret_key)\n",
    "print(\"A Session has been initialized:\", session_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_list, msg = session_instance.list_projects(project_type=\"detection\")\n",
    "\n",
    "# Check if there is a message or error\n",
    "if msg:\n",
    "    print(f\"Message: {msg}\")\n",
    "\n",
    "# Print the projects in a formatted manner\n",
    "for project_name, project_instance in projects_list.items():\n",
    "    print(f\"Project Name: {project_name} | Project ID: {project_instance.project_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"Drone-Windmill-Maintainence\"\n",
    "for name, project_instance in projects_list.items():\n",
    "    if name == project_name:\n",
    "        projects_instance = Projects(session_instance, project_name=project_name)\n",
    "        session_instance.update(projects_instance.project_id)\n",
    "        print(f\"Project '{project_name}' already exists. Initialized Projects instance.\")\n",
    "        project_exists = True\n",
    "        break\n",
    "    else:\n",
    "        project_exists = False\n",
    "\n",
    "if project_exists == False:\n",
    "    print(f\"Creating a new project: {project_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "name = \"Drone-Windmill-Maintainence\"\n",
    "\n",
    "# Function to create a project\n",
    "def create_project():\n",
    "    project_name = name\n",
    "    input_type = \"image\"\n",
    "    output_type = \"detection\"\n",
    "    \n",
    "    enabled_platforms = {\n",
    "        \"matrice\": True,\n",
    "        \"android\": False,\n",
    "        \"ios\": False,\n",
    "        \"tpu\": False,\n",
    "        \"intelCPU\": False,\n",
    "        \"gcloudGPU\": False\n",
    "    }\n",
    "\n",
    "    resp, error = session_instance._create_project(project_name, input_type, output_type)\n",
    "    if error:\n",
    "        print(f\"Error: {error}\")\n",
    "        return None, None\n",
    "    else:\n",
    "        print(f\"Project created with ID: {resp['_id']}\")\n",
    "        return resp['_id'], resp['name']\n",
    "\n",
    "# Check if project_id and project_name exist\n",
    "project_id = None\n",
    "project_name = None\n",
    "\n",
    "if not project_exists:\n",
    "    project_id, project_name = create_project()\n",
    "else:\n",
    "    project_id = projects_instance.project_id\n",
    "    project_name = projects_instance.project_name\n",
    "    print(f\"Project already exists with ID: {project_id} and Name: {project_name}\")\n",
    "\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(f\"Project ID: {project_id}\")\n",
    "print(f\"Project Name: {project_name}\")\n",
    "print(\"----------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vars.set(\"project_name\",project_name)\n",
    "Vars.set(\"project_id\", project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_instance.update(project_id)\n",
    "project_instance = Projects(session_instance, project_name=project_name)\n",
    "print(\"A Project class has been initialized\",project_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_yolo_to_coco(dataset_dir, output_dir, class_names):\n",
    "    # Prepare directories for the COCO dataset structure\n",
    "    os.makedirs(os.path.join(output_dir, 'images/train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'images/test'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'images/val'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'annotations'), exist_ok=True)\n",
    "    \n",
    "    # Define dataset parts\n",
    "    parts = ['train', 'test', 'val']\n",
    "    \n",
    "    annotation_id = 1\n",
    "    \n",
    "    # Iterate over each part (train, test, val)\n",
    "    for part in parts:\n",
    "        yolo_images_dir = os.path.join(dataset_dir, part, 'images')\n",
    "        yolo_labels_dir = os.path.join(dataset_dir, part, 'labels')\n",
    "        coco_json = {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"categories\": []\n",
    "        }\n",
    "        \n",
    "        class_ids = set()\n",
    "\n",
    "        # Process images and annotations\n",
    "        for filename in tqdm(os.listdir(yolo_labels_dir)):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                image_id = filename[:-4]  # Remove the '.txt' extension to get the full image_id\n",
    "                \n",
    "                # Construct the image path using the full image_id\n",
    "                image_path = os.path.join(yolo_images_dir, f\"{image_id}.jpg\")\n",
    "                try:\n",
    "                    # Check if the corresponding image exists\n",
    "                    if not os.path.exists(image_path):\n",
    "                        print(f\"Warning: Image file {image_path} not found, skipping.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Open the image to get its dimensions\n",
    "                    img = cv2.imread(image_path)\n",
    "                    height, width, _ = img.shape\n",
    "                    \n",
    "                    # Copy image to COCO images directory\n",
    "                    shutil.copy(image_path, os.path.join(output_dir, f'images/{part}/{image_id}.jpg'))\n",
    "                    \n",
    "                    # Add image information to COCO format\n",
    "                    coco_json[\"images\"].append({\n",
    "                        \"id\": image_id,\n",
    "                        \"file_name\": f\"{image_id}.jpg\",\n",
    "                        \"width\": width,\n",
    "                        \"height\": height\n",
    "                    })\n",
    "                    \n",
    "                    # Read YOLO annotations\n",
    "                    with open(os.path.join(yolo_labels_dir, filename), 'r') as f:\n",
    "                        for line in f.readlines():\n",
    "                            class_id, x_center, y_center, box_width, box_height = map(float, line.split())\n",
    "                            \n",
    "                            # Convert YOLO format to COCO format\n",
    "                            x_min = int((x_center - box_width / 2) * width)\n",
    "                            y_min = int((y_center - box_height / 2) * height)\n",
    "                            box_width = int(box_width * width)\n",
    "                            box_height = int(box_height * height)\n",
    "                            \n",
    "                            # Adjust class_id to start from 1 instead of 0\n",
    "                            coco_class_id = int(class_id) + 1\n",
    "                            \n",
    "                            # Add annotation to COCO format\n",
    "                            coco_json[\"annotations\"].append({\n",
    "                                \"id\": annotation_id,\n",
    "                                \"image_id\": image_id,\n",
    "                                \"category_id\": coco_class_id,\n",
    "                                \"bbox\": [x_min, y_min, box_width, box_height],\n",
    "                                \"area\": box_width * box_height,\n",
    "                                \"iscrowd\": 0\n",
    "                            })\n",
    "                            \n",
    "                            annotation_id += 1\n",
    "                            class_ids.add(coco_class_id)\n",
    "                            \n",
    "                except FileNotFoundError as e:\n",
    "                    print(f\"Warning: {e}. Skipping this file and moving on to the next.\")\n",
    "                    continue\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: An unexpected error occurred: {e}. Skipping this file and moving on to the next.\")\n",
    "                    continue\n",
    "                \n",
    "        # Add categories to COCO format\n",
    "        for class_id in sorted(class_ids):\n",
    "            coco_json[\"categories\"].append({\n",
    "                \"id\": class_id,\n",
    "                \"name\": class_names[class_id - 1],  # class_names is a list of class names indexed from 0\n",
    "                \"supercategory\": \"object\"\n",
    "            })\n",
    "        \n",
    "        # Save the COCO formatted annotations to a JSON file\n",
    "        output_json = os.path.join(output_dir, f'annotations/{part}.json')\n",
    "        with open(output_json, 'w') as f:\n",
    "            json.dump(coco_json, f, indent=4)\n",
    "        \n",
    "        print(f\"Conversion completed for {part}! COCO format saved to {output_json}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = r\"C:\\Users\\pathi\\OneDrive\\Desktop\\matriceai\\matrice-usecases\\General\\use-case-drone\\drone_highrise\"\n",
    "output_dir = r\"C:\\Users\\pathi\\OneDrive\\Desktop\\matriceai\\matrice_usecases_local\\General\\use-case-drone\\datasets\\drone_highrise_mscoco\"\n",
    "class_names = ['cable tower', 'turbine'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def convert_yolo_to_coco_safe(dataset_dir, output_dir, class_names):\n",
    "    try:\n",
    "        convert_yolo_to_coco(dataset_dir, output_dir, class_names)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Warning: {e}. Skipping this file and moving on to the next.\")\n",
    "        \n",
    "    \n",
    "convert_yolo_to_coco_safe(dataset_dir, output_dir, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import PIL\n",
    "\n",
    "# Load the COCO annotations\n",
    "with open(os.path.join(output_dir, 'annotations/train.json')) as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Create a dictionary to map image IDs to image file names\n",
    "image_id_to_file = {img['id']: img['file_name'] for img in coco_data['images']}\n",
    "\n",
    "# Create a dictionary to map category IDs to category names\n",
    "category_id_to_name = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "\n",
    "# Randomly select 5 images\n",
    "sample_images = random.sample(coco_data['images'], 5)\n",
    "\n",
    "# Plot the images with bounding boxes\n",
    "for img in sample_images:\n",
    "    img_id = img['id']\n",
    "    img_file = os.path.join(output_dir, 'images/train', img['file_name'])\n",
    "    \n",
    "    # Open the image\n",
    "    image = PIL.Image.open(img_file)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Plot the bounding boxes\n",
    "    for ann in coco_data['annotations']:\n",
    "        if ann['image_id'] == img_id:\n",
    "            bbox = ann['bbox']\n",
    "            category_id = ann['category_id']\n",
    "            category_name = category_id_to_name[category_id]\n",
    "            \n",
    "            # Create a Rectangle patch\n",
    "            rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            plt.text(bbox[0], bbox[1], category_name, bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Perform some EDA\n",
    "# Count the number of annotations per category\n",
    "category_counts = {cat['name']: 0 for cat in coco_data['categories']}\n",
    "for ann in coco_data['annotations']:\n",
    "    category_name = category_id_to_name[ann['category_id']]\n",
    "    category_counts[category_name] += 1\n",
    "\n",
    "# Plot the category distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(category_counts.keys(), category_counts.values())\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Categories in the Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Print some basic statistics\n",
    "num_images = len(coco_data['images'])\n",
    "num_annotations = len(coco_data['annotations'])\n",
    "num_categories = len(coco_data['categories'])\n",
    "\n",
    "print(f\"Number of images: {num_images}\")\n",
    "print(f\"Number of annotations: {num_annotations}\")\n",
    "print(f\"Number of categories: {num_categories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the name of the zip file\n",
    "zip_file_name = output_dir + '.zip'\n",
    "\n",
    "# Create a zip file\n",
    "shutil.make_archive(output_dir, 'zip', output_dir)\n",
    "\n",
    "print(f\"Output directory {output_dir} has been zipped into {zip_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataset_action = project_instance.import_local_dataset(\n",
    "    dataset_name='Drone-Windmill-Dataset-1',\n",
    "    file_path = r\"C:\\Users\\pathi\\OneDrive\\Desktop\\matriceai\\matrice_usecases_local\\General\\use-case-drone\\datasets\\drone_highrise_mscoco.zip\",\n",
    "    dataset_type = \"detection\",\n",
    "    dataset_description = \"Dataset of Windmill Drone Shots\",\n",
    "    version_description = \"1st version\",\n",
    "    input_type = \"image\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = dataset.dataset_id\n",
    "action_id = dataset_action.action_id\n",
    "\n",
    "print(f\"Dataset ID: {dataset_id}\")\n",
    "print(f\"Action ID: {action_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vars.set(\"dataset_id\", dataset.dataset_id)\n",
    "Vars.set(\"dataset_action_id\", dataset_action.action_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pprint\n",
    "\n",
    "action_id = dataset_action.action_id\n",
    "def check_dataset_status():\n",
    "    while True:\n",
    "        print(\" \")\n",
    "        print(\"-------Status------\")\n",
    "        D = Dataset(session_instance, dataset_id=dataset_id)\n",
    "        status = D.version_status\n",
    "        print(\"Status of dataset:\", status)\n",
    "        print(\"-------------------\")\n",
    "        if status == 'processed':\n",
    "            print(\"---------Preprocesing Complete---------\")\n",
    "            print(\"Dataset processed, proceed with experiment creation.\")\n",
    "            print(\"---------------------------------------\")\n",
    "            break\n",
    "        \n",
    "        time.sleep(90)  # Wait for 90 seconds before checking again\n",
    "\n",
    "# Run the function to check dataset status\n",
    "check_dataset_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info(dataset):\n",
    "   \n",
    "    dataset_id = dataset.dataset_details['_id']\n",
    "    dataset_name = dataset.dataset_details['name']\n",
    "    version_status = dataset.dataset_details.get(\"stats\", [{}])[0].get(\"versionStatus\")\n",
    "    latest_version = dataset.dataset_details['latestVersion']\n",
    "    no_of_samples = sum(version['versionStats']['total'] for version in dataset.dataset_details.get('stats', []))\n",
    "    no_of_classes = len(dataset.dataset_details.get('stats', [{}])[0].get('classStat', {}))\n",
    "    no_of_versions = len(dataset.dataset_details.get('allVersions', []))\n",
    "    last_updated_at = dataset.dataset_details.get('updatedAt')\n",
    "\n",
    "    print(f\"Dataset ID: {dataset_id}\")\n",
    "    print(f\"Dataset Name: {dataset_name}\")\n",
    "    print(f\"Version Status: {version_status}\")\n",
    "    print(f\"Latest Version: {latest_version}\")\n",
    "    print(f\"Number of Samples: {no_of_samples}\")\n",
    "    print(f\"Number of Classes: {no_of_classes}\")\n",
    "    print(f\"Number of Versions: {no_of_versions}\")\n",
    "    print(f\"Last Updated At: {last_updated_at}\")\n",
    "\n",
    "dataset.refresh()   \n",
    "print_dataset_info(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_summary = dataset.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "# Convert the histogram data to a DataFrame\n",
    "df = pd.DataFrame(dataset_summary['histogram'])\n",
    "\n",
    "# Bar chart for counts by category\n",
    "fig1 = go.Figure(data=[go.Bar(x=df['label'], y=df['count'])])\n",
    "fig1.update_layout(\n",
    "    title='Count of Items by Category',\n",
    "    xaxis_title='Category',\n",
    "    yaxis_title='Count'\n",
    ")\n",
    "\n",
    "# Pie chart for distribution of data items\n",
    "fig2 = go.Figure(data=[go.Pie(\n",
    "    labels=['Test', 'Train', 'Unassigned', 'Validation'],\n",
    "    values=[dataset_summary['testDataItemCount'], dataset_summary['trainDataItemCount'], dataset_summary['unassignedDataItemCount'], dataset_summary['valDataItemCount']],\n",
    "    hole=0.3\n",
    ")])\n",
    "fig2.update_layout(\n",
    "    title='Distribution of Data Items'\n",
    ")\n",
    "\n",
    "# Display the figures\n",
    "fig1.show()\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import uuid\n",
    "\n",
    "# Generate a unique experiment name\n",
    "unique_experiment_name = \"Drone-Experiment-2\"\n",
    "\n",
    "# Function to create an experiment\n",
    "def create_experiment():\n",
    "    name = unique_experiment_name\n",
    "    target_run_time = [\"PyTorch\"]\n",
    "    primary_metric = \"mAP50\"\n",
    "    matrice_compute = True\n",
    "    dataset_version = 'v1.0'\n",
    "\n",
    "    experiment_instance = project_instance.create_experiment(\n",
    "        name,\n",
    "        dataset_id,\n",
    "        target_run_time[0],\n",
    "        dataset_version,\n",
    "        primary_metric,\n",
    "        matrice_compute\n",
    "    )\n",
    "\n",
    "    print(experiment_instance)\n",
    "    experiment_id = experiment_instance.experiment_id\n",
    "    experiment_name = experiment_instance.experiment_name\n",
    "\n",
    "    print(f\"Experiment ID: {experiment_id}\")\n",
    "    print(f\"Experiment Name: {experiment_name}\")\n",
    "\n",
    "    return experiment_instance, experiment_id, experiment_name\n",
    "\n",
    "# Create a new experiment\n",
    "experiment_instance , experiment_id, experiment_name = create_experiment()\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "print(f\"Experiment Name: {experiment_name}\")\n",
    "print(f\"Experiment instance has been created: {experiment_instance}\")\n",
    "print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vars.set(\"experiment_id\", experiment_instance.experiment_id)\n",
    "Vars.set(\"experiment_name\", experiment_instance.experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matrice.model_store import _get_all_model_families \n",
    "\n",
    "resp , error , mesage = _get_all_model_families( session_instance , project_id , project_type=\"detection\" )\n",
    "print(\"----List of available Detection models on platform----\")\n",
    "# Iterate through each model family in the response\n",
    "for model_family in resp:\n",
    "    # Extract _id and modelFamily (model family name)\n",
    "    model_family_id = model_family.get('_id')\n",
    "    name = model_family.get('modelFamily')\n",
    "    \n",
    "    # Print in formatted manner\n",
    "    print(f\"ID: {model_family_id}, Model Family Name: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matrice.model_store import ModelFamily\n",
    "\n",
    "model_family_id = \"66952b139bbb09931616d4db\"\n",
    "# Initialize the ModelFamily instance after choosing a Model Family\n",
    "model_family_instance = ModelFamily(session_instance, model_family_id=model_family_id) # YOLOV8\n",
    "print(\"A ModelFamily instance has been initialized\",model_family_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch model architectures\n",
    "import pprint\n",
    "arch_resp, error, message = model_family_instance.get_model_archs()\n",
    "if error:\n",
    "    print(f\"Error: {error}\")\n",
    "else:\n",
    "    pprint.pprint(arch_resp)\n",
    "    # Check if resp is a list or a dictionary\n",
    "    if isinstance(arch_resp, list):\n",
    "        # Iterate through each model info in the list\n",
    "        for model_info in arch_resp:\n",
    "            model_key = model_info[\"model_key\"]\n",
    "            model_arch_instance = model_info[\"model_arch_instance\"]\n",
    "            \n",
    "            # Extract model information\n",
    "            model_info_id = model_arch_instance.model_info_id\n",
    "            name = model_arch_instance.model_name\n",
    "            key = model_arch_instance.model_key\n",
    "            params = model_arch_instance.params_millions\n",
    "\n",
    "            # Print in formatted manner\n",
    "            print(f\"ID: {model_info_id} |  Model Name: {name} | Model Key: {key} | Params in Millions: {params}\")\n",
    "            \n",
    "    elif isinstance(arch_resp, dict):\n",
    "        # Iterate through each model key in the dictionary\n",
    "        for model_key, model_arch_instance in arch_resp.items():\n",
    "            # Extract model information\n",
    "            model_info_id = model_arch_instance.model_info_id\n",
    "            name = model_arch_instance.model_name\n",
    "            key = model_arch_instance.model_key\n",
    "            params = model_arch_instance.params_millions\n",
    "\n",
    "            # Print in formatted manner\n",
    "            print(f\"ID: {model_info_id} | Model Name: {name} | Model Key: {key} | Params in Millions: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matrice.model_store import ModelArch\n",
    "train_arch = 'yolov8n' \n",
    "print(\"Chosen Training Architecture :\" ,train_arch)\n",
    "model_arch_instance = arch_resp.get(train_arch)\n",
    "print(\"Model Architecture instance initialized for chosen training architecture :\", model_arch_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "train_instance , train_config = model_arch_instance.get_train_config(experiment_id=Vars.get('experiment_id'))\n",
    "print(\"-----Default Train Config------\")\n",
    "pprint.pprint(train_config)\n",
    "print(\"--------------------------------\")\n",
    "print(\"-------Training instance initialized--------\")\n",
    "print(train_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = train_config['model_config']\n",
    "\n",
    "# Modify the model_config as needed\n",
    "# modifications:\n",
    "model_config['batch'] = [8]\n",
    "model_config['epochs'] = [60]\n",
    "\n",
    "\n",
    "\n",
    "# Repass the modified model_config into the train_config\n",
    "train_config['model_config'] = model_config\n",
    "\n",
    "# Print the updated train_config\n",
    "print(\"-----Updated Model Config------\")\n",
    "pprint.pprint(train_config['model_config'])\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# Print the updated train_config\n",
    "print(\"-----Updated Train Config------\")\n",
    "pprint.pprint(train_config)\n",
    "print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resp , msg , err = experiment_instance.add_models_for_training(train_instance,train_config)\n",
    "print(\"------------Model added for training----------\")\n",
    "pprint.pprint(train_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from matrice.action import Action\n",
    "\n",
    "# Define the action ID and the status you want to check\n",
    "action_id = train_resp[0]['_idActionStatus']\n",
    "success_status = 'SUCCESS'\n",
    "\n",
    "# Function to check the status of the action\n",
    "def check_action_status(S, action_id):\n",
    "    action_instance = Action(S, action_id)\n",
    "    print(\"---------------------\")\n",
    "    print(f\"Action ID: {action_id}\")\n",
    "    print(f\"Current Status: {action_instance.status}\")\n",
    "    print(f\"Step Code: {action_instance.step_code}\")\n",
    "    print(f\"Action service is : {action_instance.service_name}\")\n",
    "    print(\"---------------------\")\n",
    "    return action_instance.status\n",
    "\n",
    "# Loop to check status every 2 minutes until it is 'success'\n",
    "while True:\n",
    "    status = check_action_status(session_instance, action_id)\n",
    "    if status == success_status:\n",
    "        print(\"Action status is 'success'. Model is successfully trained.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Model is training. Waiting 10 minutes before checking again.\")\n",
    "        time.sleep(400)  # Wait for 10 minutes (600 seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = train_resp[0]['_id']\n",
    "print(f\"Model ID from response data: {model_id}\")\n",
    "    \n",
    "\n",
    "# Initialize the Model class with the model_id\n",
    "model_instance = Model(session_instance, model_id)\n",
    "print(\"A Model instance has been initialized : \", model_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the test score, validation score, and best epoch\n",
    "print(f\"Test Score: {model_instance.test_score}\")\n",
    "print(f\"Validation Score: {model_instance.val_score}\")\n",
    "print(f\"Best Epoch: {model_instance.best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vars.set(\"model_train_id\", model_instance.model_id)\n",
    "Vars.set(\"model_train_name\", model_instance.model_train_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance.plot_epochs_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance.plot_epochs_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance.plot_eval_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "# Define the folder path containing the images\n",
    "folder_path = r\"C:\\Users\\pathi\\OneDrive\\Desktop\\matriceai\\matrice_usecases_local\\datasets\\test\"\n",
    "\n",
    "\n",
    "# Get a list of all image files in the folder\n",
    "image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# Randomly select 10 images from the folder\n",
    "random_images = random.sample(image_files, 10)\n",
    "\n",
    "\n",
    "# Loop through the selected images and send the prediction request\n",
    "for image_file in random_images:\n",
    "    \n",
    "    result, error, message = model_instance.get_prediction(image_file)\n",
    "\n",
    "    if error:\n",
    "        print(f\"Error: {error}\")\n",
    "        continue\n",
    "\n",
    "    # Extract prediction data from the result\n",
    "    predictions = result\n",
    "\n",
    "    # Open the image\n",
    "    image = Image.open(image_file)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Draw bounding boxes and labels\n",
    "    for prediction in predictions:\n",
    "        category = prediction[\"category\"]\n",
    "        confidence = prediction[\"confidence\"]\n",
    "        bbox = prediction[\"bounding_box\"]\n",
    "\n",
    "        # Extract the bounding box coordinates\n",
    "        xmin, ymin, xmax, ymax = bbox[\"xmin\"], bbox[\"ymin\"], bbox[\"xmax\"], bbox[\"ymax\"]\n",
    "        width, height = xmax - xmin, ymax - ymin\n",
    "\n",
    "        # Create a Rectangle patch\n",
    "        rect = plt.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(xmin, ymin, f\"{category} ({confidence:.2f})\", bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance.download_model(file_name=\"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_config = train_instance.get_default_export_config(\"ONNX\") # Get the default export config for export format user wants (here ONNX)\n",
    "print(\"Export Config for ONNX:\")\n",
    "pprint.pprint(export_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_instance , action_instance = project_instance.create_model_export(model_id,\"ONNX\",export_config)\n",
    "\n",
    "print(\"Model Export class has been initialized :\", exported_instance )   \n",
    "print(\"----------------------------\")\n",
    "print(f\"Export ID: {exported_instance.model_export_id}\")\n",
    "print(f\"Export Name: {exported_instance.model_export_name}\")\n",
    "print(f\"Action ID: {action_instance.action_id}\")\n",
    "print(f\"Action Status: {action_instance.status}\")\n",
    "print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vars.set(\"model_export_id\", exported_instance.model_export_id)\n",
    "Vars.set(\"model_export_name\", exported_instance.model_export_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from matrice.action import Action\n",
    "\n",
    "# Define the action ID and the status you want to check\n",
    "action_id = action_instance.action_id\n",
    "success_status = 'SUCCESS'\n",
    "\n",
    "# Function to check the status of the action\n",
    "def check_action_status(S, action_id):\n",
    "    A = Action(S, action_id)\n",
    "    print(\"---------------------\")\n",
    "    print(f\"Action ID: {action_id}\")\n",
    "    print(f\"Current Status: {A.status}\")\n",
    "    print(f\"Step Code: {A.step_code}\")\n",
    "    print(f\"Action service is : {A.service_name}\")\n",
    "    print(\"---------------------\")\n",
    "    return A.status\n",
    "\n",
    "# Loop to check status every 1.5 minutes until it is 'success'\n",
    "while True:\n",
    "    status = check_action_status(session_instance, action_id)\n",
    "    if status == success_status:\n",
    "        print(\"Action status is 'success'. Model is successfully exported.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Model is exporting. Waiting 2 minutes before checking again.\")\n",
    "        time.sleep(90)  # Wait for 1.5 minutes (90 seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_instance.download_model(file_name=\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment , deployment_action = project_instance.create_deployment(\n",
    "    model_id = Vars.get('model_export_id'),  # It can also be model_train_id depending on the model you want to deploy\n",
    "    deployment_name = \"deployment_name\",\n",
    "    shutdown_threshold=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_resp = deployment.create_auth_key()\n",
    "auth_key = key_resp['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vars.set(\"deployment_id\", deployment.deployment_id)\n",
    "Vars.set(\"deployment_name\", deployment.deployment_name)\n",
    "Vars.set(\"auth_key\", auth_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "# Define the folder path containing the images\n",
    "folder_path = r\"<path_to_test_folder>\"\n",
    "\n",
    "\n",
    "# Get a list of all image files in the folder\n",
    "image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# Randomly select 10 images from the folder\n",
    "random_images = random.sample(image_files, 10)\n",
    "\n",
    "\n",
    "# Loop through the selected images and send the prediction request\n",
    "for image_file in random_images:\n",
    "    \n",
    "    result, error, message = deployment.get_prediction(auth_key,image_file)\n",
    "\n",
    "    if error:\n",
    "        print(f\"Error: {error}\")\n",
    "        continue\n",
    "\n",
    "    # Extract prediction data from the result\n",
    "    predictions = result\n",
    "\n",
    "    # Open the image\n",
    "    image = Image.open(image_file)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Draw bounding boxes and labels\n",
    "    for prediction in predictions:\n",
    "        category = prediction[\"category\"]\n",
    "        confidence = prediction[\"confidence\"]\n",
    "        bbox = prediction[\"bounding_box\"]\n",
    "\n",
    "        # Extract the bounding box coordinates\n",
    "        xmin, ymin, xmax, ymax = bbox[\"xmin\"], bbox[\"ymin\"], bbox[\"xmax\"], bbox[\"ymax\"]\n",
    "        width, height = xmax - xmin, ymax - ymin\n",
    "\n",
    "        # Create a Rectangle patch\n",
    "        rect = plt.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(xmin, ymin, f\"{category} ({confidence:.2f})\", bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
